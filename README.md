## Lá»‡nh cÃ i thÆ° viá»‡n (Python 3.10)

```bash
pip install torch numpy pandas networkx transformers gymnasium tensorboard
```


## Lá»‡nh cháº¡y train model

```bash
python -m algorithm.train_agent
```

## ğŸ§  Reinforcement Learning Environment: `TaxonomyRLEnv` (Qwen2-7B)

### ğŸ” Overview
`TaxonomyRLEnv` lÃ  má»™t mÃ´i trÆ°á»ng Reinforcement Learning (Gymnasium) cho bÃ i toÃ¡n **taxonomy navigation** trong recommendation.
Agent cÃ³ nhiá»‡m vá»¥ Ä‘iá»u hÆ°á»›ng trong **cÃ¢y taxonomy phim**, dá»±a trÃªn **lá»‹ch sá»­ xem phim cá»§a ngÆ°á»i dÃ¹ng**, nháº±m xÃ¡c Ä‘á»‹nh Ä‘Ãºng **category hoáº·c movie target**.

Agent báº¯t Ä‘áº§u tá»« **root node** vÃ  chá»n node con tá»«ng bÆ°á»›c.  
Tráº¡ng thÃ¡i Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng **LLM-based semantic embedding**.

---

### ğŸ“¥ Inputs

| ThÃ nh pháº§n | MÃ´ táº£ |
|----------|------|
| `taxonomy_tree` | CÃ¢y taxonomy phim (graph / tree) |
| `sessions_file` | File `.jsonl` chá»©a user watch sessions |
| `embedding_model` | MÃ´ hÃ¬nh sinh embedding tá»« prompt |
| `movie_details` | Metadata phim (title, description) |

---

### âš™ï¸ Environment Parameters

| Parameter | Type | Description |
|---------|------|-------------|
| `taxonomy_tree` | `TaxonomyGraph` | CÃ¢y taxonomy phim |
| `embedding_model` | `EmbeddingExtractor` | LLM-based embedding model |
| `sessions_file` | `str` | ÄÆ°á»ng dáº«n file session |
| `depth_win` | `int` / `None` | Äá»™ sÃ¢u tá»‘i Ä‘a Ä‘á»ƒ Ä‘Æ°á»£c tÃ­nh lÃ  tháº¯ng |

#### `depth_win` Logic
- `depth_win = None`: Pháº£i Ä‘áº¿n **Ä‘Ãºng movie target**
- `depth_win = k`: Chá»‰ cáº§n Ä‘i Ä‘Ãºng Ä‘Æ°á»ng Ä‘áº¿n **depth â‰¥ k**
- Náº¿u target nÃ´ng hÆ¡n `depth_win`: Pháº£i Ä‘áº¿n **Ä‘Ãºng target**

---

### ğŸ® Action Space

| Property | Value |
|--------|------|
| Type | `Discrete` |
| Size | `32` |
| Meaning | Chá»n node con tiáº¿p theo trong taxonomy |

ğŸ“Œ Náº¿u sá»‘ node con < 32 â†’ action sáº½ Ä‘Æ°á»£c **clip** vá» action há»£p lá»‡.

---

### ğŸ‘ï¸ Observation Space

| Property | Value |
|--------|------|
| Type | `Box` |
| Shape | `(4096,)` |
| Dtype | `float32` |
| Content | Semantic embedding cá»§a tráº¡ng thÃ¡i |

Observation Ä‘Æ°á»£c sinh tá»«:
- User watch history (reveal theo depth)
- Trajectory hiá»‡n táº¡i trong taxonomy
- Danh sÃ¡ch node con kháº£ dá»¥ng
- System instruction cho agent

---

### ğŸ Episode Flow

| Step | Description |
|----|-------------|
| `reset()` | Chá»n ngáº«u nhiÃªn session, reset vá» root |
| `step(action)` | Di chuyá»ƒn Ä‘áº¿n node con |
| `terminated=True` | Khi tháº¯ng, sai Ä‘Æ°á»ng hoáº·c node lÃ¡ |

---

### ğŸ§® Reward Function

| Situation | Reward | Terminated |
|----------|--------|------------|
| Correct category step | `+0.1` | âŒ |
| Reach exact movie target | `+1.0` | âœ… |
| Reach `depth_win` correctly | `+1.0` | âœ… |
| Wrong path | `-1.0` | âœ… |
| No children available | `0.0` | âœ… |

---

### â„¹ï¸ Info Field

| Key | Meaning |
|---|--------|
| `success_found_movie` | Äáº¿n Ä‘Ãºng movie |
| `success_depth_limit_reached` | Tháº¯ng do Ä‘áº¡t depth |
| `correct_step_category` | Äi Ä‘Ãºng category |
| `wrong_path` | Äi sai nhÃ¡nh |
| `no_children_available` | Node lÃ¡ |
| `action_clipped` | Action vÆ°á»£t giá»›i háº¡n |

---


## ğŸ¤– Learning Algorithm: Double DQN with Dual Replay Buffers

### ğŸ” Algorithm Overview
Thuáº­t toÃ¡n sá»­ dá»¥ng **Double DQN káº¿t há»£p Dueling Architecture**, Ä‘Æ°á»£c thiáº¿t káº¿ cho bÃ i toÃ¡n **taxonomy navigation** vá»›i khÃ´ng gian hÃ nh Ä‘á»™ng rá»i ráº¡c vÃ  tráº¡ng thÃ¡i embedding chiá»u cao.

Äáº·c Ä‘iá»ƒm chÃ­nh:
- **Double DQN**: giáº£m overestimation bias
- **Dueling Network**: tÃ¡ch Value vÃ  Advantage
- **Dual Replay Buffers**:
  - Expert Buffer (oracle-guided)
  - Online Buffer (agent interaction)
- **Curriculum Learning**: giáº£m dáº§n sá»± phá»¥ thuá»™c vÃ o expert

---

### ğŸ§  Network Architecture

| Component | Description |
|--------|-------------|
| Backbone | Dueling Deep Q-Network |
| Input | State embedding `(4096,)` |
| Output | Q-values cho tá»«ng action |
| Heads | Value stream + Advantage stream |
| Target Network | Cáº­p nháº­t Ä‘á»‹nh ká»³ |

---

### ğŸ§® Reinforcement Learning Setup

| Element | Definition |
|------|-----------|
| State (S) | LLM-based semantic embedding |
| Action (A) | Chá»n node con trong taxonomy |
| Reward (R) | Shaped reward theo path |
| Transition | `(s, a, r, s', done)` |
| Objective | Maximize expected cumulative reward |

---

### ğŸ® Action Selection

| Strategy | Description |
|-------|-------------|
| Exploration | Epsilon-Greedy |
| Exploitation | Greedy action (`argmax Q`) |
| Oracle | Expert action dá»±a trÃªn taxonomy path |

---

### ğŸ“¦ Replay Buffers

| Buffer | Size | Content |
|------|------|--------|
| Expert Buffer | `50,000` | Oracle-guided transitions |
| Online Buffer | `100,000` | Agent self-exploration |

---

### âš™ï¸ Training Hyperparameters

| Parameter | Value |
|--------|------|
| Batch Size | `256` |
| Learning Rate | `1e-4` |
| Discount Factor (Î³) | `0.99` |
| Target Update Frequency | `500` steps |
| Max Episodes | `500,000` |

---

### ğŸ¯ Exploration Parameters

| Parameter | Value |
|---------|------|
| Epsilon Start | `1.0` |
| Epsilon End | `0.05` |
| Epsilon Decay | `10,000` steps |

---

### ğŸ§ª Expert Mixing Strategy

| Parameter | Description |
|---------|-------------|
| `expert_ratio_start` | `0.5` |
| `expert_ratio_end` | `0.1` |
| `expert_decay_eps` | `5,000` episodes |

â¡ï¸ Tá»· lá»‡ sample tá»« expert buffer **giáº£m dáº§n theo episode**.

---

### ğŸ—ï¸ Training Phases

#### Phase 1 â€” Expert Prefill
| Aspect | Value |
|-----|------|
| Steps | `100,000` |
| Policy | Oracle (perfect navigation) |
| Buffer | Expert Buffer |

---

#### Phase 2 â€” Pre-training
| Aspect | Value |
|------|------|
| Updates | `200,000` |
| Data | Expert Buffer only |
| Objective | Warm-start Q-network |

---

#### Phase 3 â€” Online Training
| Aspect | Description |
|-----|------------|
| Policy | Epsilon-Greedy |
| Buffer | Mixed (Expert + Online) |
| Target Net | Periodic sync |
| Logging | TensorBoard |

---

### ğŸ“Š Evaluation Protocol

| Parameter | Value |
|--------|------|
| Eval Frequency | Every `1000` episodes |
| Eval Episodes | `100` |
| Policy | Greedy (Îµ = 0) |
| Metrics | Avg Reward, Accuracy |

**Success Conditions**:
- `success_found_movie`
- `success_depth_limit_reached`

---

### ğŸ§® Loss Function

| Component | Description |
|--------|-------------|
| Loss | Mean Squared Error (MSE) |
| Target | Double DQN target |
| Gradient Clipping | `max_norm = 1.0` |

---

### ğŸ“ˆ Logged Metrics (TensorBoard)

| Metric | Description |
|-----|-------------|
| Train/Loss | Q-learning loss |
| Train/Reward | Episode reward |
| Train/Epsilon | Exploration rate |
| Eval/Accuracy | Success rate |
| Eval/AvgReward | Avg reward on test |

---
